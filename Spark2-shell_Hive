import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
val warehouseLocation = "/user/hive/warehouse"
val spark = SparkSession.builder().appName("Spark Hive Example").config("spark.sql.warehouse.dir", warehouseLocation).enableHiveSupport().getOrCreate()
import spark.implicits._
import spark.sql
sql("select * from powerdata_minute").show()

**************************************************************

import org.apache.spark.streaming._

import org.apache.spark.streaming.kafka._

val ssc = new StreamingContext(sc, Seconds(60))

val topicMap = "TEST".split(":").map((_, 1)).toMap

val zkQuorum = "140.128.98.31:2181";

val group = "test-consumer-group"

val lines = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap).map(_._2)

lines.print()

import org.apache.spark.sql.Row
val lines_split = lines.map(x => x.split(",")).map(x => Row.fromSeq(x))

import org.apache.spark.sql.types._
val schema = StructType(List(StructField("date", StringType, true),StructField("name", StringType, true),StructField("P", StringType, true)))

import org.apache.spark.sql.SaveMode
lazy val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)

lines_split.foreachRDD(rdd =>{val DF = sqlContext.createDataFrame(rdd, schema)
DF.registerTempTable("sm")
val DF_Write = sqlContext.sql("insert into table powerdata_minute select * from sm")
DF_Write.write.mode(SaveMode.Append).saveAsTable("powerdata_minute")
})
 
//import org.apache.spark.sql.SaveMode
