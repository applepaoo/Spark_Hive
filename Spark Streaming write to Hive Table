import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka._
import org.apache.spark.sql.Row
import org.apache.spark.sql._
import org.apache.spark.sql.types.{StructType,StructField,StringType,IntegerType,TimestampType,LongType,DoubleType,DataType}
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext


val ssc = new StreamingContext(sc, Seconds(60))
val topicMap = "TEST".split(":").map((_, 1)).toMap
val zkQuorum = "140.128.98.31:2181";
val group = "test-consumer-group"
val lines = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap).map(_._2)
val lines_split = lines.map(x => x.split(",")).map( x => Row.fromSeq( x ) )
val smStruct = StructType( (0 to 2).toList.map( x => x.toString).map( y => StructField( y , StringType, true ) ) )

lines_split.foreachRDD( rdd => {
val sqlContext = new org.apache.spark.sql.hive.HiveContext(rdd.sparkContext)
val smDF = sqlContext.createDataFrame( rdd, smStruct )
smDF.registerTempTable("sm")
val smTrgPart = sqlContext.sql("insert into table powerdata_minute select * from sm")
smTrgPart.write.mode(SaveMode.Append).saveAsTable("powerdata_minute")
} )

lines_split.print()
ssc.start()
ssc.awaitTermination()
